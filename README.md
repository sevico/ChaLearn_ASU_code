brief introduction
Our code includes 4 parts. First of all, we conduct a pre-processing step. We generate 32-frame videos, Then we use two models for feature learning and extraction. We use C3D[1] to extract features of uniform sampled RGB, depth and flow data and weighted RGB data. Meanwhile, we also use Temporal segment network (TSN)[2] to extract features for depth and flow data. After that, we get multimodal features of RGB_32_train_fet,depth_32_train_fet,flow_32_train_fet,RGB_32_test_fet,depth_32_test_fet and flow_32_test_fet, RGB_section_32_train_fet and RGB_32_test_fet via C3D model, and depth_train_avg_3,depth_test_avg_3,flow_train_avg_3 and flow_test_avg_3. Then the features are blended by the below strategy: the uniform sampled RGB feature and weighted sampled RGB feature are fused by addition, as well as the depth and flow features extracted by TSN. Then these features are blended by stacking. Finally the features are used for classification by SVM classifier to get the ultimate prediction label.
======================================================================================

test environment
hardware
	CPU: Intel Core i7-6700 CPU @ 3.40GHz ¡Á8
	RAM: 16GB
	GPU: Nvidia Geforce GTX TITAN X
software
	OS: Windows 7 64-bit
	    Linux Ubuntu 14.04 LTS
	Matlab R2015b +
        opencv 3.0+ vs2015
======================================================================================
execute details
The details of purposes and steps to run of each module are revealed here.

¡î data pre-processing
We first do a pre-processing to get more robust data. First, we generate optical flow data according to [3], then we generate 32-frame videos and frame images for C3D and TSN, respectively. 

--optical flow--
The optical flow data generation code is in C3D-v1.1/pre_processing/optical_flow/. The file needs executing is optical_flow.m. Before generating the flow data, you should modify the value of variable "root", "writePath", and "feature_number". "root"is the path to the original IsoGD data. It should end with the directory of "train" or "test", for example, for the train data, it may be "./IsoGD_phase_1/train/". "writePath" is the destination of generated optical flow data. "feature_number" is the number of videos. It can be 35878 for training data or 6271 for testing data. REMEMBER TO MODIFY IT WHEN YOU CHANGE THE DATA TYPE!

--32-frame data generation--
For 32-frame videos generation, we adopt two different strategies. One is uniform sampling, in which we sampling frames uniformly, and the other is cut the videos into servral sections and sampling in terms of the variation of each section.
The uniform sampling is implemented by matlab. The code is in C3D-v1.1/pre_processing/32f_video_generate/. The file need executing is generate_src_video.m.There are 4 variables need modifying, which are "rootMK", "rootFlow" "feature_number", and "writePath". "rootMK" and "rootFlow" similar to "root" in optical_flow.m, which are the the path to RGB-D data in IsoGD and generated optical flow data. "feature_number" is same as that in optical_flow.m. However, for "writePath", as the generated data is directly used for learning, we use a relative path to put them into the C3D-v1.1/data/.
The section sampling is somewhat difficult, of which the code is in C3D-v1.1/pre_processing/section_video_generate/. we first separate the RGB and Depth data into different folders by opencv (that is the only step we use opencv and visual studio). You need to modify the varibles of "srcPath" and "destPath" in the function "make_src_video" in "make_src_video.c" to set the source path of original IsoGD data and destination path of separated data. Then the section sampling data can be generated by make_video_equal_difference.m. Also, the variables "root","flow" and "dest" need modifying for looking up the separated data obtained above, the optical flow data and for saving the generated section sampling data, respectively. Like "writePath" aforementioned, "dest" is directed to C3D-v1.1/data/.

--video to image--
The code of it is in C3D-v1.1/pre_processing/video2img/. This part is used for TSN, which need frame-level inputs. The modification-required variables "rootMK","rootFlow""feature_number", and "writePath" are same as above.


¡î model finetuning

-- C3D --
The code for finetuning C3D model is in C3D-v1.1/examples/c3d_finetuning/. When you run it, you need to access the root folder of C3D, i.e., C3D-v1.1/ and run:

bash examples/c3d_finetuning/finetuning_iso.sh MODALITY

noticing that the MODALITY is a parameter, it NEED to be replaced by rgb, depth, or flow in practice. Here we use our finetuned models, which can be downloaded and put into C3D-v1.1/models/. The snapshot caffemodels are generated per 5000 iterations, and are put into different folders in terms of the modality you chosed in C3D-v1.1/models/. 
In this step, we assuming the required data are already generated and put into corresponding folders in C3D-v1.1/data/ (e.g., the training RGB data should be in C3D-v1.1/data/train_RGB/.).

-- TSN --
run the building scripts to build the libraries.

bash build_all.sh

It will build Caffe and dense_flow. Since we need OpenCV to have Video IO, which is absent in most default installations, it will also download and build a local installation of OpenCV and use its Python interfaces.

Go to the tsn floder

To train the depth modality:  bash scripts/train_tsn.sh IsoGD depth
To train flow modality: bash scripts/train_tsn.sh IsoGD flow
feature extraction : go to scripts/feature_extraction floder,run bash extract_feature_tsn.sh

In the end, we get the needed feature.
-- C3D --
The code for feature extraction is in C3D-v1.1/examples/c3d_feature_extraction/. Like finetuning, you also need to run:

bash examples/c3d_feature_extraction/feature_extraction.sh MODALITY PHASE

in diectory C3D-v1.1/. MODALITY and PHASE are two parameters. PHASE NEED to be replaced by train or test for extracting features in different stages. The feature we extract are of the pool5 layer. They are put into different folders in pool5 (e.g., features of training depth data are in C3D-v1.1/examples/c3d_feature_extraction/pool5/depth_train/).
As these features are all .pool5 files, which are not simple to process, we use get_pool5.m, which is in the /C3D-v1.1/examples/c3d_feature_extraction/, to generate feature matrices for both training and testing features. In this step, you need to modify "feature_size" (for training or testing data) and "feature_type" (for different modalities). The matrices are directly saved into C3D-v1.1/svm_classification/features/ awaiting the final classification.

-- TSN --

* classification

Our classification is implemented by svm_fitc.m in C3D-v1.1/svm_classification/. It needs the version of matlab after 2015b since the svm toolbox is unsupported in earlier version. It can automatically load feature matrices in C3D-v1.1/svm_classification/features/. Therefore we recommend you to put the TSN features into that folder, or you need to load these features manually. 
The matrix of test_prediction (a predict label vector) is generated after classification and will be saved into C3D-v1.1/svm_classification/pred_mat/. 
According to the evaluation rule, the submission is formatted as a .txt file. Therefore the get_label_with_mat.m in the same folder may be needed to generate it. "f_unlabel_txt" needs the path to the test_list.txt file provided by the organizer. If the classification step is run properly, the test_prediction.mat can be directly loaded. Then the test_prediction.txt will be generated according to the setting "f_label_txt". 

[1] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, Learning Spatiotemporal Features with 3D Convolutional Networks, ICCV 2015.
[2] L. Wang, et al. Temporal segment networks: towards good practices for deep action recognition. ECCV 2016.
[3] T. Brox, et al. High accuracy optical flow estimation based on a theory for warping. ECCV 2004.
